{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM628COK96VJgNXaVDhdMYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f0a0837f5fe44ebcbf09400497a5b4d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c18921699974980b4029652681db607",
              "IPY_MODEL_46642541617c45879ae4072565f59d41",
              "IPY_MODEL_94e24a5ea8fa4812855d4a3f3bc5b881"
            ],
            "layout": "IPY_MODEL_1de49cc690cb428daaef2ec198f545b7"
          }
        },
        "2c18921699974980b4029652681db607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57655d05b2ef442fa2df13133525b317",
            "placeholder": "​",
            "style": "IPY_MODEL_34335d86d2564bfda4bed04b0af6c299",
            "value": "Loading weights: 100%"
          }
        },
        "46642541617c45879ae4072565f59d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f260e06b775d46ad85ef8793fc7d37a1",
            "max": 398,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8be85f76365143ea8b4e1fdc955745e3",
            "value": 398
          }
        },
        "94e24a5ea8fa4812855d4a3f3bc5b881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f9bdf80e5184ff39cb2be050aa06192",
            "placeholder": "​",
            "style": "IPY_MODEL_e6adf39c646241b3b5815c828b9cc2b7",
            "value": " 398/398 [00:00&lt;00:00, 561.68it/s, Materializing param=visual_projection.weight]"
          }
        },
        "1de49cc690cb428daaef2ec198f545b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57655d05b2ef442fa2df13133525b317": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34335d86d2564bfda4bed04b0af6c299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f260e06b775d46ad85ef8793fc7d37a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8be85f76365143ea8b4e1fdc955745e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f9bdf80e5184ff39cb2be050aa06192": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6adf39c646241b3b5815c828b9cc2b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1a8433b95c94c5da6915b36b60887c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0765ce13d404db59810b415c10abdbd",
              "IPY_MODEL_0702bc6105ee4a5182b6d89011ed6c48",
              "IPY_MODEL_7305a9d51cba4222a27cf7fc07f63c4a"
            ],
            "layout": "IPY_MODEL_b57623a54f6a440590b1dacc0e8fe0fb"
          }
        },
        "f0765ce13d404db59810b415c10abdbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8026852a00c441ae9f6b03a8c35be9cf",
            "placeholder": "​",
            "style": "IPY_MODEL_cc10c3f34f7f4984bd00a669638d9e19",
            "value": "Loading weights: 100%"
          }
        },
        "0702bc6105ee4a5182b6d89011ed6c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24f3e4196a6243b88e88c54058623786",
            "max": 398,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3227e91ea4f5433292099eb7eeb02ead",
            "value": 398
          }
        },
        "7305a9d51cba4222a27cf7fc07f63c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a950048d861b45d9a6f3ac6a500cd09d",
            "placeholder": "​",
            "style": "IPY_MODEL_a3071f8862104d0ebbe365932739ad81",
            "value": " 398/398 [00:00&lt;00:00, 577.23it/s, Materializing param=visual_projection.weight]"
          }
        },
        "b57623a54f6a440590b1dacc0e8fe0fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8026852a00c441ae9f6b03a8c35be9cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc10c3f34f7f4984bd00a669638d9e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24f3e4196a6243b88e88c54058623786": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3227e91ea4f5433292099eb7eeb02ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a950048d861b45d9a6f3ac6a500cd09d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3071f8862104d0ebbe365932739ad81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coderinf/OPTIMIZATION-OF-VLM/blob/main/VLMOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DOWNLOAD MODELS FROM HUGGING FACE TRANSFORMERS MODULE NEEDED.\n"
      ],
      "metadata": {
        "id": "3qY7hkTmfOfp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "007f43b4",
        "outputId": "5a79a112-8bae-48d5-9d10-8547ba9072fe"
      },
      "source": [
        "pip install transformers torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GET OPENAI-CLIP MODEL FROM TRANSFORMERS"
      ],
      "metadata": {
        "id": "haAZDxl7fe49"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "f0a0837f5fe44ebcbf09400497a5b4d0",
            "2c18921699974980b4029652681db607",
            "46642541617c45879ae4072565f59d41",
            "94e24a5ea8fa4812855d4a3f3bc5b881",
            "1de49cc690cb428daaef2ec198f545b7",
            "57655d05b2ef442fa2df13133525b317",
            "34335d86d2564bfda4bed04b0af6c299",
            "f260e06b775d46ad85ef8793fc7d37a1",
            "8be85f76365143ea8b4e1fdc955745e3",
            "9f9bdf80e5184ff39cb2be050aa06192",
            "e6adf39c646241b3b5815c828b9cc2b7"
          ]
        },
        "id": "1dc80ffc",
        "outputId": "19311aa2-d4d4-4168-e494-7c93499ba584"
      },
      "source": [
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Define the model name\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "# Load the pre-trained CLIP model and processor\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "model = CLIPModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(f\"Successfully loaded CLIP model: {MODEL_NAME}\")\n",
        "print(f\"Model is set to evaluation mode: {not model.training}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0a0837f5fe44ebcbf09400497a5b4d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CLIPModel LOAD REPORT from: openai/clip-vit-base-patch32\n",
            "Key                                  | Status     |  | \n",
            "-------------------------------------+------------+--+-\n",
            "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
            "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded CLIP model: openai/clip-vit-base-patch32\n",
            "Model is set to evaluation mode: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GIVE THE REQUIRED VIDEO FILE TO MODEL TO GET SEMANTICS OF EVENTS"
      ],
      "metadata": {
        "id": "1ztOblHMfy7-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1b60354",
        "outputId": "1d1854ee-f19a-4393-f5c0-bf3d10056cb4"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Define the path to the video file\n",
        "video_path = '/content/4791196-hd_1920_1080_30fps.mp4'\n",
        "\n",
        "# Check if the video file exists\n",
        "if not os.path.exists(video_path):\n",
        "    raise FileNotFoundError(f\"Video file not found at: {video_path}\")\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Check if video opened successfully\n",
        "if not cap.isOpened():\n",
        "    raise IOError(\"Error: Could not open video file.\")\n",
        "\n",
        "frames = []\n",
        "frame_count = 0\n",
        "\n",
        "# Read frames from the video\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert frame from BGR to RGB (CLIP expects RGB images)\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frames.append(frame_rgb)\n",
        "    frame_count += 1\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n",
        "\n",
        "print(f\"Successfully extracted {frame_count} frames from {video_path}\")\n",
        "print(f\"First frame shape: {frames[0].shape if frames else 'No frames extracted'}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully extracted 195 frames from /content/4791196-hd_1920_1080_30fps.mp4\n",
            "First frame shape: (1080, 1920, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GIVE LABELS FOR EVENTS REQUIRED  AND INPUT THE VIDEO TO THE MODEL\n"
      ],
      "metadata": {
        "id": "-9dRMK7WgCaP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25eb120f",
        "outputId": "da38d3ef-f6bd-4212-e16c-ae4c2de1f100"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define semantic event labels\n",
        "event_labels = [\n",
        "    'a photo of a person walking',\n",
        "    'a photo of a vehicle stopping',\n",
        "    'a photo of a crowded scene'\n",
        "]\n",
        "\n",
        "# 2. Tokenize the text labels and 3. Generate text embeddings\n",
        "with torch.no_grad():\n",
        "    text_inputs = processor(text=event_labels, return_tensors=\"pt\", padding=True)\n",
        "    # Move inputs to the same device as the model if it's on GPU\n",
        "    if torch.cuda.is_available():\n",
        "        text_inputs = {k: v.to('cuda') for k, v in text_inputs.items()}\n",
        "        model.to('cuda')\n",
        "    text_features = model.get_text_features(**text_inputs)\n",
        "    # Ensure text_features is a tensor, if it's an object, get the pooler_output\n",
        "    if hasattr(text_features, 'pooler_output'):\n",
        "        text_features = text_features.pooler_output\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# 4. Initialize an empty list to store detected events or scores\n",
        "detected_events = []\n",
        "\n",
        "print(f\"Processing {len(frames)} frames...\")\n",
        "# 5. Iterate through each frame\n",
        "for i, frame_rgb in enumerate(frames):\n",
        "    # a. Preprocess the frame\n",
        "    image_input = processor(images=frame_rgb, return_tensors=\"pt\", padding=True).pixel_values\n",
        "\n",
        "    # Move image input to the same device as the model\n",
        "    if torch.cuda.is_available():\n",
        "        image_input = image_input.to('cuda')\n",
        "\n",
        "    # b. Generate image embeddings\n",
        "    with torch.no_grad():\n",
        "        image_features = model.get_image_features(image_input)\n",
        "        # Ensure image_features is a tensor, if it's an object, get the pooler_output\n",
        "        if hasattr(image_features, 'pooler_output'):\n",
        "            image_features = image_features.pooler_output\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # c. Calculate the cosine similarity\n",
        "    similarity = torch.nn.functional.cosine_similarity(image_features, text_features)\n",
        "\n",
        "    # d. Determine the most probable event\n",
        "    best_match_idx = similarity.argmax().item()\n",
        "    best_match_score = similarity[best_match_idx].item()\n",
        "    predicted_event = event_labels[best_match_idx]\n",
        "\n",
        "    # e. Store the frame index, the detected event, and its similarity score.\n",
        "    detected_events.append({\n",
        "        'frame_index': i,\n",
        "        'predicted_event': predicted_event,\n",
        "        'similarity_score': best_match_score\n",
        "    })\n",
        "\n",
        "# 6. Print or display the detected events for each frame (e.g., first 10 and last 10)\n",
        "print(\"\\n--- Detected Events (First 10) ---\")\n",
        "for event in detected_events[:10]:\n",
        "    print(f\"Frame {event['frame_index']}: {event['predicted_event']} (Score: {event['similarity_score']:.4f})\")\n",
        "\n",
        "if len(detected_events) > 10:\n",
        "    print(\"\\n...\")\n",
        "    print(\"\\n--- Detected Events (Last 10) ---\")\n",
        "    for event in detected_events[-10:]:\n",
        "        print(f\"Frame {event['frame_index']}: {event['predicted_event']} (Score: {event['similarity_score']:.4f})\")\n",
        "\n",
        "print(f\"\\nCompleted event detection for {len(detected_events)} frames.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 195 frames...\n",
            "\n",
            "--- Detected Events (First 10) ---\n",
            "Frame 0: a photo of a crowded scene (Score: 0.2159)\n",
            "Frame 1: a photo of a crowded scene (Score: 0.2197)\n",
            "Frame 2: a photo of a crowded scene (Score: 0.2286)\n",
            "Frame 3: a photo of a crowded scene (Score: 0.2278)\n",
            "Frame 4: a photo of a crowded scene (Score: 0.2314)\n",
            "Frame 5: a photo of a crowded scene (Score: 0.2288)\n",
            "Frame 6: a photo of a crowded scene (Score: 0.2278)\n",
            "Frame 7: a photo of a crowded scene (Score: 0.2266)\n",
            "Frame 8: a photo of a crowded scene (Score: 0.2262)\n",
            "Frame 9: a photo of a vehicle stopping (Score: 0.2249)\n",
            "\n",
            "...\n",
            "\n",
            "--- Detected Events (Last 10) ---\n",
            "Frame 185: a photo of a person walking (Score: 0.2395)\n",
            "Frame 186: a photo of a person walking (Score: 0.2360)\n",
            "Frame 187: a photo of a person walking (Score: 0.2292)\n",
            "Frame 188: a photo of a person walking (Score: 0.2312)\n",
            "Frame 189: a photo of a person walking (Score: 0.2319)\n",
            "Frame 190: a photo of a person walking (Score: 0.2377)\n",
            "Frame 191: a photo of a person walking (Score: 0.2399)\n",
            "Frame 192: a photo of a person walking (Score: 0.2460)\n",
            "Frame 193: a photo of a person walking (Score: 0.2481)\n",
            "Frame 194: a photo of a person walking (Score: 0.2540)\n",
            "\n",
            "Completed event detection for 195 frames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CALCULATE THE BENCHMARKS FOR THE ACTUAL VLM."
      ],
      "metadata": {
        "id": "iiZYe0E8gmOn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37bc2913",
        "outputId": "6af8110e-b393-491f-8a62-02fed87b56af"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Ensure event_labels, processor, model, text_features, and frames are available from previous steps\n",
        "\n",
        "# 1. Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Re-initialize detected_events list (optional, if you want to keep the output clean for this step)\n",
        "detected_events_performance = []\n",
        "\n",
        "print(f\"Benchmarking inference speed for {len(frames)} frames...\")\n",
        "\n",
        "# 2. Iterate through each frame\n",
        "for i, frame_rgb in enumerate(frames):\n",
        "    # a. Preprocess the frame\n",
        "    image_input = processor(images=frame_rgb, return_tensors=\"pt\", padding=True).pixel_values\n",
        "\n",
        "    # Move image input to the same device as the model\n",
        "    if torch.cuda.is_available():\n",
        "        image_input = image_input.to('cuda')\n",
        "\n",
        "    # b. Generate image embeddings\n",
        "    with torch.no_grad():\n",
        "        image_features = model.get_image_features(image_input)\n",
        "        # Ensure image_features is a tensor, if it's an object, get the pooler_output\n",
        "        if hasattr(image_features, 'pooler_output'):\n",
        "            image_features = image_features.pooler_output\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # c. Calculate the cosine similarity\n",
        "    similarity = torch.nn.functional.cosine_similarity(image_features, text_features)\n",
        "\n",
        "    # d. Determine the most probable event (keeping this for consistency, though not strictly needed for benchmarking)\n",
        "    best_match_idx = similarity.argmax().item()\n",
        "    best_match_score = similarity[best_match_idx].item()\n",
        "    predicted_event = event_labels[best_match_idx]\n",
        "\n",
        "    # e. Store the frame index, the detected event, and its similarity score. (optional for performance metrics)\n",
        "    detected_events_performance.append({\n",
        "        'frame_index': i,\n",
        "        'predicted_event': predicted_event,\n",
        "        'similarity_score': best_match_score\n",
        "    })\n",
        "\n",
        "# 3. Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# 4. Calculate the total inference time\n",
        "total_inference_time = end_time - start_time\n",
        "\n",
        "# 5. Calculate the average inference time per frame\n",
        "average_time_per_frame = total_inference_time / len(frames)\n",
        "\n",
        "# 6. Calculate the Frames Per Second (FPS)\n",
        "fps = len(frames) / total_inference_time\n",
        "\n",
        "# 7. Print the results\n",
        "print(\"\\n--- Baseline Model Performance Metrics ---\")\n",
        "print(f\"Total frames processed: {len(frames)}\")\n",
        "print(f\"Total inference time: {total_inference_time:.4f} seconds\")\n",
        "print(f\"Average inference time per frame: {average_time_per_frame:.4f} seconds\")\n",
        "print(f\"Frames Per Second (FPS): {fps:.2f}\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking inference speed for 195 frames...\n",
            "\n",
            "--- Baseline Model Performance Metrics ---\n",
            "Total frames processed: 195\n",
            "Total inference time: 46.9261 seconds\n",
            "Average inference time per frame: 0.2406 seconds\n",
            "Frames Per Second (FPS): 4.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "APPLY QUANTIZATION TO THE VLM TO INT8"
      ],
      "metadata": {
        "id": "MDRZPRJJgydh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b4e841a",
        "outputId": "e4ba924b-b166-47bc-9f31-189949925e06"
      },
      "source": [
        "import torch\n",
        "from torch.quantization import quantize_dynamic\n",
        "import os\n",
        "\n",
        "# Ensure the model is on CPU for dynamic quantization\n",
        "# Dynamic quantization for CLIP typically works best on CPU\n",
        "model.cpu()\n",
        "\n",
        "print(f\"Applying dynamic quantization to model of type {type(model)}...\")\n",
        "\n",
        "# 1. Apply dynamic quantization to the loaded model\n",
        "# Quantize only linear layers, which are common in CLIP's text and vision transformers\n",
        "quantized_model = quantize_dynamic(model,\n",
        "                                   {\n",
        "                                       torch.nn.Linear\n",
        "                                   },\n",
        "                                   dtype=torch.qint8,\n",
        "                                   inplace=False)\n",
        "\n",
        "print(\"Dynamic quantization applied successfully.\")\n",
        "\n",
        "# 2. Save the quantized_model\n",
        "output_model_path = 'quantized_clip_model.pt'\n",
        "torch.save(quantized_model.state_dict(), output_model_path)\n",
        "\n",
        "print(f\"Quantized model saved to {output_model_path}\")\n",
        "\n",
        "# Compare model sizes\n",
        "# To get the size of the original model, save its state_dict to a temporary file\n",
        "original_model_temp_path = 'original_clip_model_temp.pt'\n",
        "torch.save(model.state_dict(), original_model_temp_path)\n",
        "original_model_size_mb = os.path.getsize(original_model_temp_path) / (1024 * 1024)\n",
        "os.remove(original_model_temp_path) # Clean up the temporary file\n",
        "\n",
        "quantized_model_size_mb = os.path.getsize(output_model_path) / (1024 * 1024)\n",
        "\n",
        "print(f\"Original model size: {original_model_size_mb:.2f} MB\")\n",
        "print(f\"Quantized model size: {quantized_model_size_mb:.2f} MB\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying dynamic quantization to model of type <class 'transformers.models.clip.modeling_clip.CLIPModel'>...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-206427606.py:13: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = quantize_dynamic(model,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic quantization applied successfully.\n",
            "Quantized model saved to quantized_clip_model.pt\n",
            "Original model size: 577.21 MB\n",
            "Quantized model size: 224.46 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD THE OPTIMIZED MODEL\n"
      ],
      "metadata": {
        "id": "vtBNvD9jhf2S"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364,
          "referenced_widgets": [
            "d1a8433b95c94c5da6915b36b60887c5",
            "f0765ce13d404db59810b415c10abdbd",
            "0702bc6105ee4a5182b6d89011ed6c48",
            "7305a9d51cba4222a27cf7fc07f63c4a",
            "b57623a54f6a440590b1dacc0e8fe0fb",
            "8026852a00c441ae9f6b03a8c35be9cf",
            "cc10c3f34f7f4984bd00a669638d9e19",
            "24f3e4196a6243b88e88c54058623786",
            "3227e91ea4f5433292099eb7eeb02ead",
            "a950048d861b45d9a6f3ac6a500cd09d",
            "a3071f8862104d0ebbe365932739ad81"
          ]
        },
        "id": "e6a5aa08",
        "outputId": "4e5ff2d5-92ac-4b7a-faf0-cb83470c5ec2"
      },
      "source": [
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "# Define the model name used for loading the original model structure\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "# Re-initialize the original model (on CPU as it was quantized on CPU)\n",
        "# This is necessary to load the state_dict into the correct model architecture\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "original_model_for_quantization = CLIPModel.from_pretrained(MODEL_NAME)\n",
        "original_model_for_quantization.cpu()\n",
        "\n",
        "# Re-apply quantization to get the quantized model object structure\n",
        "# We need to recreate the quantized model structure to load its state_dict\n",
        "quantized_model_loaded = quantize_dynamic(original_model_for_quantization,\n",
        "                                           {torch.nn.Linear},\n",
        "                                           dtype=torch.qint8,\n",
        "                                           inplace=False)\n",
        "\n",
        "# Load the state_dict of the saved quantized model\n",
        "output_model_path = 'quantized_clip_model.pt'\n",
        "quantized_model_loaded.load_state_dict(torch.load(output_model_path))\n",
        "\n",
        "# Set the loaded quantized model to evaluation mode\n",
        "quantized_model_loaded.eval()\n",
        "\n",
        "print(f\"Successfully loaded quantized model from {output_model_path}\")\n",
        "print(f\"Quantized model is set to evaluation mode: {not quantized_model_loaded.training}\")\n",
        "\n",
        "# Make the loaded quantized model globally available for subsequent benchmarking\n",
        "model_quantized = quantized_model_loaded"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1a8433b95c94c5da6915b36b60887c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CLIPModel LOAD REPORT from: openai/clip-vit-base-patch32\n",
            "Key                                  | Status     |  | \n",
            "-------------------------------------+------------+--+-\n",
            "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
            "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "/tmp/ipython-input-3622332936.py:16: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model_loaded = quantize_dynamic(original_model_for_quantization,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded quantized model from quantized_clip_model.pt\n",
            "Quantized model is set to evaluation mode: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PASS THE SAME VIDEO AS INPUT TO THE OPTIMIZED MODEL"
      ],
      "metadata": {
        "id": "74yooukdhlhO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "460c318d",
        "outputId": "79c49af7-f79c-4a5d-e2b5-6eeaf20091b6"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Ensure event_labels, processor, frames are available from previous steps\n",
        "# Make sure `model_quantized` is defined from the previous step\n",
        "\n",
        "# Re-generate text features using the *quantized* model's text encoder\n",
        "# This is crucial as quantization affects the model's internal representation\n",
        "print(\"Re-generating text features using the quantized model's text encoder...\")\n",
        "with torch.no_grad():\n",
        "    text_inputs_quantized = processor(text=event_labels, return_tensors=\"pt\", padding=True)\n",
        "    # Ensure text inputs are on CPU as the quantized model is on CPU\n",
        "    text_features_quantized = model_quantized.get_text_features(**text_inputs_quantized)\n",
        "    if hasattr(text_features_quantized, 'pooler_output'):\n",
        "        text_features_quantized = text_features_quantized.pooler_output\n",
        "    text_features_quantized /= text_features_quantized.norm(dim=-1, keepdim=True)\n",
        "print(\"Text features for quantized model re-generated.\")\n",
        "\n",
        "# 1. Record the start time\n",
        "start_time_quantized = time.time()\n",
        "\n",
        "# Re-initialize detected_events list for quantized model performance (optional)\n",
        "detected_events_performance_quantized = []\n",
        "\n",
        "print(f\"Benchmarking inference speed for quantized model on {len(frames)} frames...\")\n",
        "\n",
        "# 2. Iterate through each frame\n",
        "for i, frame_rgb in enumerate(frames):\n",
        "    # a. Preprocess the frame\n",
        "    image_input = processor(images=frame_rgb, return_tensors=\"pt\", padding=True).pixel_values\n",
        "    # Ensure image input is on CPU for the quantized model\n",
        "    # image_input = image_input.to('cpu') # It is already on CPU by default from processor\n",
        "\n",
        "    # b. Generate image embeddings using the quantized model\n",
        "    with torch.no_grad():\n",
        "        image_features_quantized = model_quantized.get_image_features(image_input)\n",
        "        # Ensure image_features is a tensor, if it's an object, get the pooler_output\n",
        "        if hasattr(image_features_quantized, 'pooler_output'):\n",
        "            image_features_quantized = image_features_quantized.pooler_output\n",
        "        image_features_quantized /= image_features_quantized.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # c. Calculate the cosine similarity\n",
        "    similarity_quantized = torch.nn.functional.cosine_similarity(image_features_quantized, text_features_quantized)\n",
        "\n",
        "    # d. Determine the most probable event (keeping for consistency)\n",
        "    best_match_idx_quantized = similarity_quantized.argmax().item()\n",
        "    best_match_score_quantized = similarity_quantized[best_match_idx_quantized].item()\n",
        "    predicted_event_quantized = event_labels[best_match_idx_quantized]\n",
        "\n",
        "\n",
        "    # e. Store the frame index, the detected event, and its similarity score. (optional\n",
        "    detected_events_performance_quantized.append({\n",
        "        'frame_index': i,\n",
        "        'predicted_event': predicted_event_quantized,\n",
        "        'similarity_score': best_match_score_quantized\n",
        "    })\n",
        "\n",
        "# 3. Record the end time\n",
        "end_time_quantized = time.time()\n",
        "\n",
        "# 4. Calculate the total inference time\n",
        "total_inference_time_quantized = end_time_quantized - start_time_quantized\n",
        "\n",
        "# 5. Calculate the average inference time per frame\n",
        "average_time_per_frame_quantized = total_inference_time_quantized / len(frames)\n",
        "\n",
        "# 6. Calculate the Frames Per Second (FPS)\n",
        "fps_quantized = len(frames) / total_inference_time_quantized\n",
        "\n",
        "# 7. Print the results\n",
        "print(\"\\n--- Quantized Model Performance Metrics ---\")\n",
        "print(f\"Total frames processed: {len(frames)}\")\n",
        "print(f\"Total inference time: {total_inference_time_quantized:.4f} seconds\")\n",
        "print(f\"Average inference time per frame: {average_time_per_frame_quantized:.4f} seconds\")\n",
        "print(f\"Frames Per Second (FPS): {fps_quantized:.2f}\")\n",
        "\n",
        "# Store metrics for later comparison table\n",
        "metrics_quantized = {\n",
        "    \"total_frames\": len(frames),\n",
        "    \"total_inference_time\": total_inference_time_quantized,\n",
        "    \"average_time_per_frame\": average_time_per_frame_quantized,\n",
        "    \"fps\": fps_quantized\n",
        "}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-generating text features using the quantized model's text encoder...\n",
            "Text features for quantized model re-generated.\n",
            "Benchmarking inference speed for quantized model on 195 frames...\n",
            "\n",
            "--- Quantized Model Performance Metrics ---\n",
            "Total frames processed: 195\n",
            "Total inference time: 32.1149 seconds\n",
            "Average inference time per frame: 0.1647 seconds\n",
            "Frames Per Second (FPS): 6.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FORMAT THE COMPARISION TABLE"
      ],
      "metadata": {
        "id": "C3OXWUVUh5iU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3391545",
        "outputId": "502a9b3d-a7a1-4647-f61c-2e3141013fff"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Collect baseline model performance metrics\n",
        "# These variables are available from the previous execution of cell 37bc2913\n",
        "baseline_total_inference_time = total_inference_time\n",
        "baseline_average_time_per_frame = average_time_per_frame\n",
        "baseline_fps = fps\n",
        "\n",
        "# 2. Collect optimized (quantized) model performance metrics\n",
        "# These variables are available from the previous execution of cell 460c318d\n",
        "quantized_total_inference_time = metrics_quantized[\"total_inference_time\"]\n",
        "quantized_average_time_per_frame = metrics_quantized[\"average_time_per_frame\"]\n",
        "quantized_fps = metrics_quantized[\"fps\"]\n",
        "\n",
        "# Also retrieve model sizes from previous steps (cell 1b4e841a)\n",
        "original_model_size_mb = original_model_size_mb\n",
        "quantized_model_size_mb = quantized_model_size_mb\n",
        "\n",
        "# 3. Create a pandas DataFrame for the comparison table\n",
        "comparison_data = {\n",
        "    'Metric': [\n",
        "        'Total Frames Processed',\n",
        "        'Total Inference Time (s)',\n",
        "        'Average Time per Frame (s)',\n",
        "        'Frames Per Second (FPS)',\n",
        "        'Model Size (MB)'\n",
        "    ],\n",
        "    'Baseline Model': [\n",
        "        len(frames),\n",
        "        f'{baseline_total_inference_time:.4f}',\n",
        "        f'{baseline_average_time_per_frame:.4f}',\n",
        "        f'{baseline_fps:.2f}',\n",
        "        f'{original_model_size_mb:.2f}'\n",
        "    ],\n",
        "    'Optimized Model': [\n",
        "        len(frames),\n",
        "        f'{quantized_total_inference_time:.4f}',\n",
        "        f'{quantized_average_time_per_frame:.4f}',\n",
        "        f'{quantized_fps:.2f}',\n",
        "        f'{quantized_model_size_mb:.2f}'\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Calculate speedup metrics\n",
        "inference_time_reduction = (baseline_total_inference_time - quantized_total_inference_time) / baseline_total_inference_time * 100\n",
        "fps_speedup_factor = quantized_fps / baseline_fps\n",
        "model_size_reduction = (original_model_size_mb - quantized_model_size_mb) / original_model_size_mb * 100\n",
        "\n",
        "print(\"--- Performance Comparison Table ---\")\n",
        "print(comparison_df.to_markdown(index=False))\n",
        "\n",
        "print(f\"\\nObserved Speedup (FPS factor): {fps_speedup_factor:.2f}x\")\n",
        "print(f\"Inference Time Reduction: {inference_time_reduction:.2f}%\")\n",
        "print(f\"Model Size Reduction: {model_size_reduction:.2f}%\")\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Performance Comparison Table ---\n",
            "| Metric                     |   Baseline Model |   Optimized Model |\n",
            "|:---------------------------|-----------------:|------------------:|\n",
            "| Total Frames Processed     |         195      |          195      |\n",
            "| Total Inference Time (s)   |          46.9261 |           30.2969 |\n",
            "| Average Time per Frame (s) |           0.2406 |            0.1554 |\n",
            "| Frames Per Second (FPS)    |           4.16   |            6.44   |\n",
            "| Model Size (MB)            |         577.21   |          224.46   |\n",
            "\n",
            "Observed Speedup (FPS factor): 1.55x\n",
            "Inference Time Reduction: 35.44%\n",
            "Model Size Reduction: 61.11%\n"
          ]
        }
      ]
    }
  ]
}